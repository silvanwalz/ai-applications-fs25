{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Model Selection und Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lernziele\n",
    "\n",
    "* Hyperparameter Tuning: systematisches Ausprobieren verschiedener Hyperparameter.\n",
    "* Bias/Variance Tradeoff: Analyse der Performance In-Sample und Out-of-Sample in Abhängigkeit der Modellkomplexität.\n",
    "* k-fold Cross-Validation: Sophistiziertere Art der Validierung um statistische Schwankungen zu reduzieren.\n",
    "* Bestimmen optimaler Hyperparameter(-kombinationen) mit Hilfe von Cross-Validation.\n",
    "* Split in Trainings-, Validation-, und Testset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hat man einen Machine Learning Algorithmus gewält, muss er üblicherweise noch genauer spezifiziert werden durch Wahl von sogenannten **Hyperparametern**. Beim *kNN-Algorithmus* haben wir solche Hyperparameter schon gesehen: z.B. die Anzahl der zu betrachtenden Nachbarn (das 'k' in kNN) oder die zu verwendende Metrik. Praktisch jeder ML-Algorithmus hat eine Reihe solcher Hyperparameter. \n",
    "\n",
    "Einige Beispiele von Hyperparametern verschiedener ML-Algorithmen (nur Auszugsweise, bei weitem nicht abschliessend):\n",
    "* (Lineare oder Logistische) Regression: Funktionale Form der Schätzgleichung\n",
    "* Entscheidungsbäume: die Tiefe des Baumes\n",
    "* Neuronale Netze: die Tiefe und der Aufbau des Netzes, die verwendete Aktivierungsfunktion\n",
    "\n",
    "Die Hyperparameter werden **nicht** während des Trainingsprozesses (mit der *fit()* Methode von scikit-learn) gelernt, sondern **vor** dem Trainieren mit den Trainingsdaten von Hand **festgelegt**. In scikit-learn setzt man die Hyperparameter, indem man beim Aufruf des verwendenten Machine Learning Modells die Parameter als Argumente übergibt. Werden keine Argumente übergeben, werden sogennante *Default*-Werte übernommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel zum Hyperparameter Tuning: Klassifikation von simulierten Daten mittels kNN mit unterschiedlicher Anzahl berücksichtigter nächster Nachbarn\n",
    "\n",
    "Wir verwenden hier einen neuen Datensatz mit simulierten Daten. In diesem Datensatz ist jeder Datenpunkt durch zwei Features 'x1' und 'x2' charakterisiert. Der Label 'y' nimmt entweder die Werte 0 oder 1 an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Folgenden werden wir die \"üblichen\" Pakete benötigen:\n",
    "import numpy as np # Python Paket zum Umgang mit Vektoren und anderen mathematischen Funktionalitäten\n",
    "import matplotlib.pyplot as plt # grundlegendes Python Paket um Plots zu erstellen\n",
    "import pandas as pd # Python Paket zum Umgang mit Datensätzen\n",
    "import seaborn as sns # Python Paket für \"schöne\" Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiermit legen wir die Grösse von Plots fest:\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)  # in den runden Klammern wird die Breite und Höhe der Plots angegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Datensatzes als Pandas DataFrame und erste Übersicht\n",
    "daten = pd.read_csv('SimData.csv', index_col = 0)\n",
    "daten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphische Darstellung des Datensatzes\n",
    "sns.scatterplot(x = 'x1', y = 'x2', data = daten, hue = 'y', legend = False)\n",
    "# Bemerkung: legend=False \"schaltet\" die (allenfalls störende) Anzeige der Legende aus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle/Modellvarianten definieren\n",
    "Wir verwenden ein kNN Modell mit 3 nächsten Nachbarn (*model3*) und eines mit 15 (*model15*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = KNeighborsClassifier(n_neighbors = 3)  # Argument n_neighbors explizit festlegen\n",
    "model3.get_params()   # model3 ist nun ein kNN-Algorithmus, der 3 nächste Nachbarn berücksichtigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = KNeighborsClassifier(n_neighbors = 15)  # Argument n_neighbors explizit festlegen\n",
    "model15.get_params()   # model15 ist nun eine kNN-Algorithmus, der 15 nächste Nachbarn berücksichtigt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split des Datensatzes in ein *Trainingsset* (benutzt, um die beiden Modelle zu trainieren) und eine *Validationset* (benutzt, um die Out-of-Sample Performance der Modelle zu ermitteln). Durch Vergleich der Out-of-Sample Performance der beiden Modelle **lernen** wir, welches der beiden Modelle besser geeignet ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TVSplit](TrainValSplit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features und Labels definieren und in Trainings- und Validationset splitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features sind die Spalten 'x1' und 'x2', der Labels ist die Spalte 'y'\n",
    "X = daten[['x1','x2']].values  # Feature Matrix\n",
    "y = daten['y'].values   # Vektor der Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in Trainingsset (X_t,y_t) und Validationset (X_v, y_v)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_v, y_t, y_v = train_test_split(X,y, test_size = 0.25, stratify = y, random_state = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren der beiden Modelle\n",
    "model3.fit(X_t,y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15.fit(X_t,y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Sample Scores\n",
    "print('In-Sample:')\n",
    "print('Score Modell 1:', model3.score(X_t,y_t))\n",
    "print('Score Modell 2:', model15.score(X_t,y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-Sample Scores\n",
    "print('Out-of-Sample:')\n",
    "print('Score Modell 1:', model3.score(X_v,y_v))\n",
    "print('Score Modell 2:', model15.score(X_v,y_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse der unterschiedlichen Wirkungsweise der beiden Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um besser zu verstehen, was vor sich geht, eignet sich eine Visualisierung der sogenannten **Decision-Boundary** (\"Entscheidungs-Grenze\"): sie zeigt, welchen Bereichen im x1-x2-Streudiagramm (d.h. für welche Werte der Features) der Algorithmus welche Klasse zuordnet.\n",
    "\n",
    "Im Folgenden nutzen wir die **NICHT-prüfungsrelevante** Funktion *plot_dec_bound(clf, X, y)*, um diese Visualisierung zu erstellen. Diese Funktion wird in der nächsten Zelle definiert. Sie müssen diese Funktion **nicht** verstehen. Sie brauchen den Code in dieser Zelle **nicht** durchzulesen. Aber sie **müssen** diese Zelle (wie üblich) mit SHIFT-ENTER **ausführen**. Diese Funktion hat drei Argumente:\n",
    "* *clf*: das zur Klassifikation zu verwendende Modell\n",
    "* *X*: die Features der Daten des Trainingssets\n",
    "* *y*: die Labels der Daten des Trainingssets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier wird die Funktion plot_dec_bound(clf, X, y) definiert\n",
    "# Diese Zelle ist NICHT prüfungsrelevant!\n",
    "# Sie müssen den Code in dieser Zelle NICHT durchgehen.\n",
    "# Hilfsfunktion zur Visualisierung der Decision Boundary\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def plot_dec_bound(clf, X, y):\n",
    "    LE = LabelEncoder()\n",
    "    LE.fit(y)\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF']) #, '#AAAAFF'])\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF']) # , '#0000FF'])\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = LE.transform(clf.predict(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading = 'auto')\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=LE.transform(y), cmap=cmap_bold, s = 10)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schauen uns nun mit Hilfe der soeben definierten Funktion die Streudiagramme und **Decision-Boundaries** der beiden Modelle an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsset und resultierende Decision-Boundary des kNN-Modells mit 3 nächsten Nachbarn\n",
    "plot_dec_bound(model3, X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsset und resultierende Decision-Boundary des kNN-Modells mit 15 nächsten Nachbarn\n",
    "plot_dec_bound(model15, X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "* Die Decision-Boundary des Modells mit 3 nächsten Nachbarn ist deutlich unruhiger oder zackiger als beim Modell mit 15 nächsten Nachbarn. Einbezug von mehr nächsten Nachbarn *glättet* also die Decision-Boundary.\n",
    "* Die Decision-Boundary des Modells mit 3 nächsten Nachbarn identifiziert etliche \"blaue\" Ausreisser innerhalb des \"roten\" Gebiets. Das führt zu einer besseren In-Sample Performance. Allerdings werden neue Datenpunkte dadurch wahrscheinlich öfters falsch klassifiziert, was zu einer kleineren Out-of-Sample Performance führt.\n",
    "* Insgesamt sieht die Decision-Boundary des Modells mit 15 nächsten Nachbarn \"vernünftiger\" aus. Wenn wir \"von Hand\" eine Decision-Boundary in das Streudiagramm der Trainingsdaten hätten einzeichnen müssen, dann hätten wir das wahrscheinlich eher so wie beim Modell mit den 15 Nachbarn gezeichnet. Diese Grenze scheint gut die Gebiete in denen *üblicherweise* (bis auf einzelne *Ausreisser*) die blauen und roten Punkte liegen zu trennen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: Systematisches Ausprobieren vieler verschiedener Modelle und/oder Parameterwerte\n",
    "\n",
    "Oben haben wir für den Hyperparameter zwei mögliche Werte eingesetzt und gesehen, dass das eine Modell bessere Resultate liefert als das andere. Ziel des Hyperparameter Tuning ist es, den Spielraum der möglichen Hyperparameter-Werte möglichst systematisch und vollständig auszuloten, um denjenigen zu finden, der zu den besten Resultaten führt.\n",
    "\n",
    "In unserem Fall bedeutet dies, dass wir systematisch Analysieren wollen, welche Anzahl nächster Nachbarn die beste Out-of-Sample Genauigkeit liefert. Wir gehen daher eine ganze **Liste** von potentiell sinnvollen Parameterwerten durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Liste der auszuprobierenden Werte für den Parameter n_neighbors.\n",
    "klist = [1,3,5,7,9,11,13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 41, 45, 49, 57, 65, 73, 89, 105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen nun eine Python *for*-Schleife um diese Liste der Parameter durchzugehen und für jeden Wert\n",
    "- das kNN-Modell mit der entsprechenden Anzahl nächster Nachbarn definieren\n",
    "- das Modell auf dem Trainingsset trainieren\n",
    "- die In-Sample (auf dem Trainingsset) und Out-of-Sample (auf den Validationset) Performance messen\n",
    "\n",
    "Die Perormance Kennzahlen der verschiedenen Modelle sammeln wir in zwei Listen *insamp* und *outsamp*, die wir beim Durchlaufen der Schleife nach und nach aufbauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorbereitung der Listen, in denen die Resultate der In- und Out-of-Sample Genauigkeit gesammelt werden sollen\n",
    "insamp = []  # Liste für die In-Sample Genauigkeit\n",
    "outsamp = [] # Liste für die Out-of-Sample Genauigkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In einer for-Schleife die Werte der Hyperparameter durchgehen und die Resultate (In- und Out-of-Sample scores)\n",
    "# in den Listen insamp (für Trainingsset) und outsamp (für Testset) sammeln\n",
    "for k in klist:\n",
    "    model = KNeighborsClassifier(n_neighbors = k)  # Modell definiern\n",
    "    model.fit(X_t,y_t)                             # Modell auf dem Trainingsset trainieren \n",
    "    insamp.append(model.score(X_t,y_t))         # resultierende In-Sample Performance an die Liste insamp anhängen\n",
    "    outsamp.append(model.score(X_v,y_v))        # resultierende Out-of-Sample Performance an die Liste outsamp anhängen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswertung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schaen uns zunächst die Performancekennzahlen (Accuracy) der verschiedenen Modellvarianten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-Sample Accuracies\n",
    "insamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-Sample Accuracies\n",
    "outsamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anstatt die Zahlen direkt zu betrachten, können wir viel verstehen, was vor sich geht, wenn wir die In- und Out-of-Sample Accuracy in einem Linienplot (*sns.lineplot(x,y)*) als Funktion der Anzahl nächster Nachbarn darstellen. Auf der x-Achse sind die verwendeten Parameterwerte (k=1 bis k=105) abgetragen. Auf der y-Achse ist die resultierende In-Sample (blaue Linie) und Out-of-Sample (rote Linie) Vorhersagegenauigkeit abgetragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = klist, y = insamp, color ='blue')   # Linie der In-Sample scores\n",
    "sns.lineplot(x = klist, y = outsamp, color = 'red')  # Linie der Out-of-Sample scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit *matplotlib* Funktionen können wir dieses Diagramm noch \"verschönern\" indem wir die Achsen beschriften und einen Titel hinzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit matplotlib.pyplot Funktionen können wir das Diagramm noch beschriften\n",
    "# import matplotlib.pyplot as plt\n",
    "plt.title('In-Sample (blau) und Out-of-Sample (rot) Vorhersagegenauigkeit') # Titel des Diagramms\n",
    "plt.xlabel('Anzahl Nachbarn') # Titel der x-Achse\n",
    "plt.ylabel('Genauigkeit') # Titel der y-Achse\n",
    "sns.lineplot(x = klist, y = insamp, color ='blue')\n",
    "sns.lineplot(x = klist, y = outsamp, color = 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beobachtungen**:\n",
    "\n",
    "* Die Out-of-Sample Genauigkeit ist (praktisch) immer kleiner als die In-Sample Genauigkeit.\n",
    "* Die **In-Sample Genauigkeit** zeigt einen (ziemlich) **monotonen** Verlauf: Je komplexer das Modell (je kleiner das k), desto besser die In-Sample Genauigkeit. Im Extremfall k=1 erreichen wir 100% In-Sample Genauigkeit.\n",
    "* Die **Out-of-Sample Genauigkeit** steigt, erreicht ein **Maximum** und fällt dann wieder. \"Extreme\" Modelle (mit sehr kleinem k oder sehr grossem k) führen **beide** zu einer schlechten Out-of-Sample Genauigkeit. Das beste Modell mit der grössten Out-of-Sample Genauigkeit liegt irgendwo **in der Mitte**. In diesem Beispiel bei k ingendwo zwischen etwa 15 und 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detaillierte Analyse der In-Sample und Out-of-Sample Performance als Funktion des Hyperparameter k (Anzahl nächster Nachbarn)\n",
    "\n",
    "Wir wollen den Verlauf der Performance-Kurven im obigen Bild besser verstehen. Dazu sehen wir uns **drei** Fälle genauer an:\n",
    "* *model_Over*: Kleinster verwendeter Hyperparameter *n_neighbors=1*\n",
    "* *model_Under*: Grösster verwendeter Hyperparameter *n_neighbors=105*\n",
    "* *model_Opt*: Modell mit der grössten Out-of-Sample Genauigkeit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Halbautomatisches) Finden des Modells mit der besten Out-of-Sample Genauigkeit\n",
    "Für das *model_Opt* müssen wir zunächst das k mit der grössten Out-of-Sample Genauigkeit finden. Dieses k könnte man natürlich \"von Hand\" aus der obigen Grafik ablesen, aber man kann es auch mit Hife entsprechender Python/numpy-Funktionen \"automatisch\" finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst die LISTE der Out-of-Sample Genauigkeiten outsamp in einen NUMPY nd.array konvertieren\n",
    "outarray = np.array(outsamp)\n",
    "outarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit der Methode max() finden wir in einem ndarray den grössten Wert\n",
    "print('Grösster Wert im Array:', outarray.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit der Methode argmax() finden wir die INDEXPOSITION des grössten Wertes im Array\n",
    "print('Indexposition des grössten Wertes im Array:', outarray.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt müssen wir nur noch herausfinden, welchen Wert für k wir an dieser Indexposition ausprobiert haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Liste der ausprobierten k-Werte:\n",
    "klist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der k Wert des Modells an der Indexposition 6\n",
    "klist[outarray.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenfassung\n",
    "print('Grössste erreichte Out-of-Sample Genauigkeit:', outarray.max())\n",
    "print('Modell mit der besten Out-of-Sample Genauigkeit: k =', klist[outarray.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse der drei Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelle definieren\n",
    "model_Over = KNeighborsClassifier(n_neighbors = 1) # kleinstes k\n",
    "model_Under = KNeighborsClassifier(n_neighbors = 105) # grösstes k\n",
    "model_Opt = KNeighborsClassifier(n_neighbors = 13) # \"optimales\" k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelle auf Trainingsset trainieren\n",
    "model_Over.fit(X_t, y_t)\n",
    "model_Under.fit(X_t, y_t)\n",
    "model_Opt.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell mit k = 1\n",
    "plot_dec_bound(model_Over, X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell mit k = 105\n",
    "plot_dec_bound(model_Under, X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell mit k = 13\n",
    "plot_dec_bound(model_Opt, X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beobachtungen**:\n",
    "* Je kleiner das k, desto komplexere Strukturen kann das Modell nachbilden: ein **kleines k** entspricht also einem **komplexen Modell**, ein **grosses k** einem **einfachen Modell**. Modelle mit kleinem k zeigen eine wild gezackte, aufgesplitterte Decision-Boundary. Modelle mit (sehr) grossem k zeigen eine ganz glatte Decision-Boundary.\n",
    "* **Problem** bei sehr einfachen Modellen (sehr grosses k, hier k = 105): sie sind **zu unflexibel** und können die in den (Trainings-)Daten enthaltenen Strukturen **nicht gut** nachbilden. Man spricht hier von **Underfitting** bzw. sagt zu einfache Modelle haben einen **grossen Bias** (d.h. \"sie liegen tendentiell immer falsch\").\n",
    "* **Problem** bei sehr komplexen Modellen (sehr kleines k, hier k = 1): sie sind **zu flexibel** und bilden **alle** Strukturen in den Trainingsdaten (exakt) nach, **auch** das in den Traniningsdaten enthaltene **Rauschen (Noise)**. Jeder *Ausreisser* wird als *systematisch* empfunden. Das *Noise*/die *Ausreisser* sind aber von Datensatz zu Datensatz völlig unterschiedlich, und insbesondere im Validation-Set auch völlig anders als im Trainings-Set. Deshalb die schlechte Out-of-Sample Performance dieser Modelle. Man spricht hier von **Overfitting** bzw. sagt, zu komplexe Modelle haben eine **hohe Varianz** (d.h. \"sie zeigen von Sample zu Sample völlig unterschiedliches Verhalten\").\n",
    "* Das **beste Modell** ist das mit der **grössten Out-of-Sample Genauigkeit** und liegt (wie so oft) irgendwo in der Mitte (hier bei k = 13). Es hat meist einen *mittleren* Komplexitätsgrad: genügend komplex, um Strukturen die von Datensatz zu Datensatz reproduzierbar sind, nachzubilden; aber genügend einfach, um nicht auf das nicht-reproduzierbare Noise 'hineinzufallen'. Man spricht hier von einem optimalen **Tradeoff** zwischen Overfitting und Underfitting, bzw. einem optimalen Tradeoff zwischen möglichst kleinem **Bias** und möglichst kleiner **Variance**. Dies nennt man den **Bias-Variance-Tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grosses Achtung:\n",
    "\n",
    "### In-Sample Overfitting (also die Wahl von Modellen mit hoher Varianz) ist einer der am häufigsten gemachten Fehler im Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diskussion\n",
    "\n",
    "Wir beurteilen die im ersten Teil untersuchten beiden Modelle mit 3 und mit 15 nächsten Nachbarn anhand folgender Kriterien:\n",
    "\n",
    "Das *model3* bzw. *model15*...\n",
    "- ist komplex/einfach/optimal\n",
    "- zeigt overfitting/underfitting/optimalen Tradeoff zwischen over- und underfitting\n",
    "- ist zu flexibel/zu unflexibel/gerade richtig\n",
    "- hat eine grosse Varianz/kleine Varianz\n",
    "- hat einen grossen Bias/kleinen Bias\n",
    "- bildet nicht-reproduzierbares Noise nach/nicht nach\n",
    "- kann die reproduzierbaren Strukturen in den Daten abbilden/nicht abbilden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Stabilere\" Ergebnisse durch Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meist unterliegen die ermittelten Performance Kennzahlen (z.B. die Vorhersagegenauigkeit) im Validationset spürbaren statistischen Schwankungen (siehe z.B. das recht wilde 'hin-und-her-zappeln' der Out-of-Sample Vorhersagegenauigkeit als Funktion des Hyperparameter k, die rote Linie, im Bild oben). Gleichzeitig sind die Unterschiede in der Out-of-Sample Performance bei unterschiedlichen Hyperparametern oft recht klein, so dass man oft kaum unterscheiden kann, ob eine scheinbar bessere Performance rein auf statistischen Schwankungen beruht oder tatsächlich 'nachhaltig' ist.\n",
    "\n",
    "Die statistischen Schwankungen liessen sich am besten reduzieren, wenn man mehr Daten hätte. Leider ist es aber oft nicht möglich, an (substantiell) mehr Daten zu kommen. Eine Lösung, die mit den verfügbaren Daten auskommt, ist die sogenannte **Cross-Validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel: 5-fache Cross-Validation (\"5-fold Cross-Validation\")\n",
    "Das Sample wird in 5 gleich grosse Teile aufgeteilt. Es werden nun 5 Durchläufe (*Runs*) durchgeführt; d.h. es wird fünf mal auf dem Trainingsteil (vier fünftel der Daten) trainiert und dann die Out-of-Sample Performance auf dem Validationset (das übrige fünftel der Daten) gemessen. Bei jedem Run wird ein anderes Füntel des Datensatzes als Validationset genommen. Im Bild unten sind die 5 Teile des Datensatzes und die 5 Runs gezeigt. Der gelbe Block repräsentiert dabei für jeden Run jeweils das Validationset, die grünen Teile das Trainingsset.\n",
    "\n",
    "Als (Zwischen-)ergebnis erhält man für jeden Run die In-Sample (im Trainingsset) und Out-of-Sample (im Validationset) Performance. Schlussendlich werden jeweils die fünf In-Sample und die fünf Out-of-Sample Performance Kennzahlen **gemittelt**, um eine *präzisere* Schätzung der In- und Out-of-Sample Performance zu erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CVSplit](CrossVal5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Umsetzung in scikit-learn**:\n",
    "\n",
    "Die scikit-learn Funktion für die Cross-Validierung ist *cross_validate()*. Diese Funktion führt **völlig automatisch** das oben beschriebene Prozedere durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Funktion aus scikit-learn importieren\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir spielen die Cross-Validation einmal im Detail durch für ein kNN-Modell mit $k = 3$ Nachbarn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Modell definieren\n",
    "model = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So benutzt man die Funktion zur Cross-Validation\n",
    "scores = cross_validate(model, X, y, cv = 5, return_train_score = True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für uns wichtige Parameter der Funktion *cross_validate()*:\n",
    "\n",
    "* erster Parameter: das zu evaluierende Modell, hier *model*\n",
    "* zweite und dritte Parameter: Feature-Matrix und Labels des (gesamten) Samples, hier X und y\n",
    "* *cv=*: Die Anzahl der Splits, hier *cv = 5*, d.h. Aufteilen des gesamten Samples in 5 Teile (Splits)\n",
    "* *return_train_score=*: Falls \"True\" gibt die Funktion sowohl die Out-of-Sample (auf dem Validationset berechnete) **als auch** die In-Sample (auf dem Trainingsset berechnete) Performance zurück\n",
    "\n",
    "Als Ergebnis liefert die Funktion *cross_validate()* ein **Dictionary** mit den für die Berechnung benötigten Rechenzeiten (für uns irrelevant) und den Trainings- und Test-Scores. Alle Einträge in diesem Dictionary sind numpy ndarrays, die für jeden der fünf Durchläufe die entprechenden Grössen (Rechenzeiten/Scores) festhalten.\n",
    "\n",
    "**Bemerkung**: Standardmässig liefert die Funktion *cross_validate()* in den Teilen *test_score* und *train_score* dasjenige Performance Mass, welches durch die *score()* Methode des Modells berechnet wird. Bei Klassifikationsmodellen (wie z.B. hier das kNN-Modell) ist das üblicherweise die Vorhersageganauigkeit (accuracy).\n",
    "\n",
    "**Beachten Sie**: Innerhalb der Funktion *cross_validate()* werden mehrere Schritte abgearbeitet:\n",
    "- die als Parameter übergebenen Features und Labels werden in 5 Teile gesplittet,\n",
    "- es werden die oben genannten 5 Runs durchgeführt\n",
    "- für jeden Run wird das als Parameter übergebene Modell auf dem Trainigsteil trainiert,\n",
    "- für jeden Run wird auf dem Trainings- und Validationteil die Performance gemessen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswertung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst aus dem Dictionary 'scores' die 5 Out-of-Sample Scores der 5 Runs als numpy array auslesen\n",
    "scores['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **Mittelwert** der scores der einzelnen fünf Durchläufe ist der *cross-validated* score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Den Mittelwert der 5 Out-of-Sample scores bestimmen wir mit der numpy Methode mean()\n",
    "scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusammenfassung**: die *cross-validated* In- und Out-of-Sample Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated In- und Out-of-Sample Scores\n",
    "print('In Sample Score:', scores['train_score'].mean())\n",
    "print('Out-of-Sample Score:', scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exkurs: Cross-Validation bei *geordneten* Datensätzen.\n",
    "\n",
    "Mit dem Argument 'cv = 5' in der Funktion *cross_validate()* wird der Datensatz **genau der Reihe nach** in die 5-Teile gesplittet (das erste 5tel des Datensatzes kommt in den ersten Teil, das zweite in den zweiten etc.). Wenn die Daten im Datensatz **ungeordnet**, gut durchmischt vorliegen, ist dieses Vorgehen korrekt. Sind die Daten im Datensatz jedoch **geordnet** (z.B. der Grösse einer Variablen nach, oder nach Klassen), so liefert ein Split der Reihe nach **falsche** Ergebnisse.\n",
    "\n",
    "Die **Lösung bei geordneten Datensätzen** ist, diese **vor dem Split zu mischen**. Dies geschieht durch die Angabe des Parameters 'cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 5)'. Beispiel:\n",
    "\n",
    "*scores = cross_validate(model, X, y, cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 5), return_train_score = True)*\n",
    "\n",
    "Die Anzahl der Splits und der 'random_state' können dabei natürlich beliebig festgelegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vollständige Analyse mittels Cross-Validation\n",
    "\n",
    "Wir überprüfen heir nochmals die Qualität von kNN Modellen mit Hyperparameter k zwischen 1 und 105, wobei wir aber *Cross-Validation* verwenden, um die Modell-Performance zu ermitteln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier gehen wir wie oben die Liste der auszuprobierenden Hyperparameter k in einer Schleife durch\n",
    "# und sammeln die durch Cross-Validation ermittelten Performance Kennzahlen in zwei Listen insamp_cv und outsamp_cv\n",
    "insamp_cv = []\n",
    "outsamp_cv = []\n",
    "for k in klist:\n",
    "    model = KNeighborsClassifier(n_neighbors = k)\n",
    "    scores = cross_validate(model, X, y, cv = 5, return_train_score = True) # Wir verwenden hier 5-fache CV\n",
    "    insamp_cv.append(scores['train_score'].mean())\n",
    "    outsamp_cv.append(scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Ergebnisse\n",
    "plt.title('In-Sample (blau) und Out-of-Sample (rot) Cross-Validated Genauigkeit (dicke Linien)')\n",
    "plt.xlabel('n_neighbors = k')\n",
    "plt.ylabel('Genauigkeit')\n",
    "# Die Cross-validated Scores\n",
    "sns.lineplot(x = klist, y = insamp_cv, color ='blue', linewidth = 2)\n",
    "sns.lineplot(x = klist, y = outsamp_cv, color = 'red', linewidth = 2)\n",
    "# Zum Vergleich, als dünne Linien noch die Scores von oben, ohne Cross-Validation\n",
    "sns.lineplot(x = klist, y = insamp, color ='blue', linewidth = 0.5)\n",
    "sns.lineplot(x = klist, y = outsamp, color = 'red', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beobachtung**:\n",
    "\n",
    "- Der Verlauf der *cross-validated* In-Sample und ganz besonders der Out-of-Sample Scores erscheint nun deutlich *glatter/gleichmässiger* als die mit einem einzigen Validationset ermittelten Werte. Durch die Cross-Validation wurden die statistischen Fluktuationen etwas reduziert, was auch das Ziel war. Die bereits oben erkannte Struktur bleibt dabei erhalten und wird deutlicher sichtbar.\n",
    "- Wir sehe auch, dass es wohl kein eindeutiges, otpimales k gibt. Gute Werte für k sind wohl alle (ungeraden) Werte zwischen etwa 19 und 29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das k des Modells mit der besten Out-of-Sample Cross-Validated Performance\n",
    "klist[np.array(outsamp_cv).argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abschluss: Train-, Validation- und Testset\n",
    "\n",
    "Oben haben wir die auf dem Validationset ermittete 'Out-of-Sample' Performance zum **Lernen** der Hyperparameter benutzt. Da damit die im Validationset vorhandenen Daten bereits im Lernprozess verwendet wurden, ist die auf dem Validationset gemessene Out-of-Sample Performance **kein guter Indikator** dafür, wie der am Schluss gewählte Algorithmus auf **gänzlich neuen** Daten performen wird. Um eine gute Schätzung für die Performance auf neuen Daten zu erhalten, müssten Daten vorhanden sein, die **in keinster Weise** im Lernprozess verwendet wurden.\n",
    "\n",
    "Das vollständige Vorgehen (Umgang mit den Vorhandenen Daten) ist daher Folgendes:\n",
    "\n",
    "* die vorhandenen Daten zunächst in **zwei** Teile aufteilen: einen Teil, der im Lernprozess verwendet wird, und einen Teil, der - ohne jemals angeschaut zu werden - als **Testset** bis zum Schluss 'zur Seite gelegt wird'.\n",
    "* der Datensatz für den Lernprozess wird wie oben genutzt, um das Modell zu fitten (auf den Trainingsdaten) und ggf. Hyperparameter zu lernen (mittels Validationset oder Cross-Validation).\n",
    "* Ist der Lernprozess **vollständig abgeschlossen** wird die Performance des Modells auf den bis hierhin noch gänzlich unbenutzten Daten des Testsets gemessen. Die auf dem Testset ermittelte Performance ist damit eine guter Schätzwert für die Performance des Modells auf neuen Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tvt](TrainValTestSplit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
